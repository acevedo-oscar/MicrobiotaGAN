{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctDJSqGeDwcE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir= os.getcwd()\n",
    "\n",
    "# %autoreload makes Jupyter to reload modules before executing the cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../src\")\n",
    "from MicrobiotaGAN.generator import Generator\n",
    "from MicrobiotaGAN.discriminator import Discriminator\n",
    "from MicrobiotaGAN.cost import wasserstein_generator_cost\n",
    "from MicrobiotaGAN.cost import wasserstein_discriminator_cost\n",
    "from MicrobiotaGAN.input_noise_sample import input_noise_sample\n",
    "from MicrobiotaGAN.dataset_manager import DataSetManager \n",
    "from MicrobiotaGAN.glv_loss import GLV_Model\n",
    "from MicrobiotaGAN.utilities import *\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjxuHDiOzSos"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    " \n",
    "sns.set()\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ITP_Ru3mUQR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "microbiota_table = pd.read_csv('../data/abundances.csv',header=None  ).values\n",
    "\n",
    " \n",
    "    \n",
    "param_path = '../data/image_dynamics_param.pickle' \n",
    "\n",
    "with open(param_path, 'rb') as pickleFile:\n",
    "    microbiota_parameters = pickle.load(pickleFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdbt1KNvPWSZ"
   },
   "source": [
    "# Splitting data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-E1a3zuVDuk8",
    "outputId": "0713e61b-7952-40e1-e433-3fcf21107a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set(1000, 50)\n",
      "Train set: (9000, 50)\n",
      "\n",
      "\n",
      "Sigma: 0.6\n",
      "Rho: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Load Data to the data mangaer\n",
    "\n",
    "# Reduce data amount for quick training\n",
    "\n",
    "microbiota_table = microbiota_table[0:10000]\n",
    "#microbiota_table = np.multiply(microbiota_table, 0.1)\n",
    "\n",
    "np.random.shuffle(microbiota_table)\n",
    "\n",
    "training_percent = 0.9\n",
    "\n",
    "lim = int(np.ceil(microbiota_table.shape[0]*training_percent))\n",
    "train_set = microbiota_table[0:lim,:]\n",
    "test_set = microbiota_table[lim:,:]\n",
    "print(\"Test set\" +str(test_set.shape))\n",
    "print(\"Train set: \"+str(train_set.shape))\n",
    "\n",
    "\n",
    "microbiota_test_set =  DataSetManager(test_set, norm=True)\n",
    "microbiota_train_set=  DataSetManager(train_set, norm=True)\n",
    "\n",
    "m_sigma = microbiota_parameters['sigma']\n",
    "m_rho = microbiota_parameters['rho']\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Sigma: \"+str(m_sigma))\n",
    "print(\"Rho: \"+str(m_rho))\n",
    "\n",
    "m_A = microbiota_parameters['A']\n",
    "\n",
    "m_r = microbiota_parameters['r']\n",
    "\n",
    "n_species = m_A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Um1NYiQCGGkT",
    "outputId": "c2fb0f70-b660-4887-a01c-6d76e48acd43"
   },
   "outputs": [],
   "source": [
    "#np.std(mbio_dict[\"max_vec\"], ddof=1) \n",
    "#x = microbiota_test_set.next_batch(1)\n",
    "#print(np.sum(x[0,:]))\n",
    "#print(np.max(x[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ijsQ9Coq1BmH",
    "outputId": "866a67c7-e279-463a-8591-fb034426b103"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['N', 'rho', 'sigma', 'mu', 'T_f', 'A', 'r'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "microbiota_parameters.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycVfLbhzJYRD"
   },
   "source": [
    "# Dataset exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "PVoMy1tuw6x3",
    "outputId": "7752267a-0d53-4f1e-8fee-c900f197792d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance from origin: 0.6082762530298219\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.axis(xmax=1, ymax=1)\n",
    "plt.plot(m_sigma, m_rho,'ro')\n",
    " \n",
    "plt.xlabel(r'Interspecies interaction [$\\sigma$]') \n",
    "plt.ylabel(r'Interspecies probability [$\\rho$]')\n",
    "\n",
    "print(\"Distance from origin: \"+ str(np.sqrt(m_rho**2 + m_sigma**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "id": "SchkL5cj3WEp",
    "outputId": "fb8d654e-6fec-4db8-b5ce-b17234580c36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'A Matrix')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(microbiota_parameters['A'], cmap=\"gray_r\")\n",
    "plt.title(\"A Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "colab_type": "code",
    "id": "CTRQ5tMk3029",
    "outputId": "c7cd9393-f998-48a4-9a11-1dcffa767eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    }
   ],
   "source": [
    "# Micorbiota abundance pseudo histogram\n",
    "print(list(range(1,m_A.shape[0])))\n",
    "\n",
    "sns.barplot(x = list(range(1,m_A.shape[0]+1)) ,y= np.sum(normalize_ds(microbiota_table), axis=0))\n",
    "plt.title(\"Dataset Community Normalized Species Abundance\")\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Abundance\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNZdMZoVJT0X"
   },
   "source": [
    "# Computational Graph Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djyTAm2V1FjJ"
   },
   "outputs": [],
   "source": [
    "def train_graph(my_discriminator, my_generator, n_species,noise_dim ):\n",
    "\n",
    "    # Inputs\n",
    "    train_real_sample = tf.placeholder(tf.float32, shape=[None, n_species], name=\"train_real_sample\")\n",
    "    train_noise_sample = tf.placeholder(tf.float32, shape=[None, noise_dim], name=\"train_noise_sample\")     \n",
    "\n",
    "    # Compute Logits\n",
    "    D_real, norm_logit_t_real,  D_logit_real_train,  real_L1_output = my_discriminator.train_probability_and_logit(train_real_sample)\n",
    "\n",
    "    fake_gan_samples = my_generator.train_draw_samples(train_noise_sample)\n",
    "    D_fake_train,  norm_logit_t_fake,  D_logit_fake_train, fake_L1_output = my_discriminator.train_probability_and_logit(fake_gan_samples)\n",
    "\n",
    "    # Compute Cost\n",
    "    D_cost_train =  wasserstein_discriminator_cost(D_logit_real_train, D_logit_fake_train)\n",
    "    G_cost_train =  wasserstein_generator_cost( D_logit_fake_train)\n",
    "\n",
    "    # Define Optimize Step\n",
    "    D_train_step = my_discriminator.optimize_step(D_cost_train)\n",
    "    G_train_step = my_generator.optimize_step(G_cost_train)\n",
    "\n",
    "    # Others\n",
    "    clip_D = my_discriminator.clip_parameters(0.01)\n",
    "\n",
    "    return [ train_real_sample, train_noise_sample,  G_cost_train,\n",
    "     G_train_step, D_cost_train, D_train_step,  D_logit_fake_train,\n",
    "      D_logit_real_train,clip_D, norm_logit_t_real,  D_logit_real_train,  real_L1_output, norm_logit_t_fake,  D_logit_fake_train, fake_L1_output ]\n",
    "\n",
    "def inference_graph(my_discriminator, my_generator, n_species,noise_dim ):\n",
    "\n",
    "    # Inputs\n",
    "    inference_real_sample = tf.placeholder(tf.float32, shape=[None, n_species])\n",
    "    inference_noise_sample = tf.placeholder(tf.float32, shape=[None, noise_dim])\n",
    "\n",
    "    # Compute Logits\n",
    "    D_real, norm_logit_inf_real,  D_logit_real = my_discriminator.inference_probability_and_logit(inference_real_sample)\n",
    "\n",
    "    generator_sample_inference = my_generator.inference_draw_samples(inference_noise_sample)\n",
    "    D_fake_inference, norm_logit_inf_fake, D_logit_fake_inference = my_discriminator.inference_probability_and_logit(generator_sample_inference)\n",
    "\n",
    "    D_cost_inference =  wasserstein_discriminator_cost(D_logit_real, D_logit_fake_inference)\n",
    "    G_cost_inference = wasserstein_generator_cost( D_logit_fake_inference)\n",
    " \n",
    "\n",
    "    # Others\n",
    "    clip_D = my_discriminator.clip_parameters(0.01)\n",
    "\n",
    "    return [ inference_real_sample, inference_noise_sample, G_cost_inference, D_cost_inference, clip_D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_species : int = m_A.shape[0]\n",
    "mini_batch_size : int =32\n",
    "\n",
    "noise_dim : int = 10\n",
    "noise_sample = tf.placeholder(tf.float32, shape=[None, noise_dim])\n",
    "\n",
    "\n",
    "# Computation Graph Definition\n",
    "my_generator = Generator(noise_dim, n_species)\n",
    "my_discriminator = Discriminator(n_species)\n",
    "\n",
    " \n",
    "train_real_sample, train_noise_sample,  G_cost_train, G_train_step, D_cost_train, D_train_step,  D_logit_fake_train, D_logit_real_train,clip_D, norm_logit_t_real,  D_logit_real_train,  real_L1_output, norm_logit_t_fake,  D_logit_fake_train, fake_L1_output = train_graph(my_discriminator, my_generator, n_species,noise_dim )\n",
    " \n",
    "\n",
    "# inference_real_sample, inference_noise_sample, G_cost_inference, D_cost_inference, clip_D= inference_graph(my_discriminator, my_generator, n_species,noise_dim )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ihlo7X_fFzCB"
   },
   "outputs": [],
   "source": [
    "# Initialize the network graph\n",
    "# sess = tf.InteractiveSession() # tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mROsD_yXR52e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"train_real_sample:0\", shape=(?, 50), dtype=float32)\n",
      "####\n",
      "name: \"RMSProp_1\"\n",
      "op: \"NoOp\"\n",
      "input: \"^RMSProp_1/update_G_W1/ApplyRMSProp\"\n",
      "input: \"^RMSProp_1/update_G_W2/ApplyRMSProp\"\n",
      "input: \"^RMSProp_1/update_G_b1/ApplyRMSProp\"\n",
      "input: \"^RMSProp_1/update_G_b2/ApplyRMSProp\"\n",
      "\n",
      "####\n",
      "Tensor(\"Neg:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(train_real_sample)\n",
    "print(\"####\")\n",
    "\n",
    "print(G_train_step)\n",
    "print(\"####\")\n",
    "print(G_cost_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7tDdnS5lFOIf"
   },
   "source": [
    "#Train Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1251
    },
    "colab_type": "code",
    "id": "QD8OUEGc1Cza",
    "outputId": "7d636d8b-70dc-4897-ba9f-fa0634b5eb65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso :9.9644128113879 %\n",
      "3.0597738359974755 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.212929\n",
      "Epochs completed so far 1\n",
      "\t Iter: 112\n",
      "\t D loss: 0.17\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.6673317\n",
      "L1 Output Fake \t549.28296\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t-0.22253174\n",
      "L1 Output Real \t122.81926\n",
      "Progreso :19.9288256227758 %\n",
      "5.04345678000027 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.600174\n",
      "Epochs completed so far 3\n",
      "\t Iter: 224\n",
      "\t D loss: 0.1758\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.667861\n",
      "L1 Output Fake \t559.71655\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t-0.03897578\n",
      "L1 Output Real \t140.59657\n",
      "Progreso :29.893238434163703 %\n",
      "6.796181755998987 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.632397\n",
      "Epochs completed so far 5\n",
      "\t Iter: 336\n",
      "\t D loss: 0.1795\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.668089\n",
      "L1 Output Fake \t560.4712\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t0.079285234\n",
      "L1 Output Real \t149.98325\n",
      "Progreso :39.8576512455516 %\n",
      "9.19167160500001 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.615574\n",
      "Epochs completed so far 7\n",
      "\t Iter: 448\n",
      "\t D loss: 0.1768\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.667901\n",
      "L1 Output Fake \t560.0762\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t-0.00653054\n",
      "L1 Output Real \t143.24515\n",
      "Progreso :49.8220640569395 %\n",
      "11.02130079699782 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.583258\n",
      "Epochs completed so far 9\n",
      "\t Iter: 560\n",
      "\t D loss: 0.1799\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.6681128\n",
      "L1 Output Fake \t559.3207\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t0.09338172\n",
      "L1 Output Real \t139.17062\n",
      "Progreso :59.786476868327405 %\n",
      "13.001603983997484 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.689047\n",
      "Epochs completed so far 11\n",
      "\t Iter: 672\n",
      "\t D loss: 0.1783\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.667001\n",
      "L1 Output Fake \t561.7964\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t0.040964074\n",
      "L1 Output Real \t144.18306\n",
      "Progreso :69.7508896797153 %\n",
      "15.094556085998192 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.60229\n",
      "Epochs completed so far 13\n",
      "\t Iter: 784\n",
      "\t D loss: 0.1774\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.668188\n",
      "L1 Output Fake \t559.7644\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t0.013255481\n",
      "L1 Output Real \t147.16252\n",
      "Progreso :79.7153024911032 %\n",
      "17.332992446998105 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.634552\n",
      "Epochs completed so far 15\n",
      "\t Iter: 896\n",
      "\t D loss: 0.1776\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.6679144\n",
      "L1 Output Fake \t560.52026\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t0.018642997\n",
      "L1 Output Real \t149.81332\n",
      "Progreso :89.6797153024911 %\n",
      "20.078975819000334 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.609398\n",
      "Epochs completed so far 17\n",
      "\t Iter: 1008\n",
      "\t D loss: 0.1807\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.6682596\n",
      "L1 Output Fake \t559.9309\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t0.116980165\n",
      "L1 Output Real \t153.5314\n",
      "Progreso :99.644128113879 %\n",
      "22.01910078599758 s\n",
      "\t Current Gen Weights 6.0715494\n",
      "\t Current Dis Weights 22.597588\n",
      "Epochs completed so far 19\n",
      "\t Iter: 1120\n",
      "\t D loss: 0.1829\n",
      "\t G_loss: 0.1771\n",
      "\n",
      "\n",
      "Norm Logit Fake \t1600.0\n",
      "Logit Fake \t-5.6685047\n",
      "L1 Output Fake \t559.6549\n",
      "Norm Logit Real \t1600.0\n",
      "Logit Real \t0.18704613\n",
      "L1 Output Real \t139.71945\n"
     ]
    }
   ],
   "source": [
    "number_of_fig_per_plot :int = 16\n",
    "# Training Loop\n",
    "counter = 0\n",
    "d_iter_ratio: int = 5\n",
    "\n",
    "train_epochs = 20\n",
    "\n",
    "iters_per_epoch = train_set.shape[0]//mini_batch_size\n",
    "\n",
    "iteration_number = train_epochs*iters_per_epoch\n",
    "# Since the discriminator is trained 5 times more, de divide the number of iterations\n",
    "iteration_number = int(np.ceil(iteration_number/d_iter_ratio))\n",
    "\n",
    "start = timer()\n",
    "\n",
    "train_g_cost_record = []\n",
    "train_d_cost_record = []\n",
    "iter_record_g = []\n",
    "iter_record_d = []\n",
    "\n",
    "epoch_record_g = []\n",
    "epoch = []\n",
    "\n",
    "g_train_epoch_cost = []\n",
    "d_train_epoch_cost =[]\n",
    " \n",
    "\n",
    "initial_epochs = microbiota_train_set.epochs_completed\n",
    "\n",
    "little_record = []\n",
    "\n",
    "add_g_record = 0\n",
    "D_current_cost = 0\n",
    "G_current_cost = 0\n",
    "\n",
    "gen_logit = []\n",
    "dis_logit = []\n",
    "\n",
    "#### Sess 1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    it = 0\n",
    "    while microbiota_train_set.epochs_completed - initial_epochs < train_epochs:\n",
    "\n",
    "        it += 1\n",
    "\n",
    "        if microbiota_train_set.epochs_completed == 2:\n",
    "            little_record.append(sess.run(my_generator.G_W1))\n",
    "\n",
    "        # Train more the discriminator\n",
    "        for k in range(d_iter_ratio):      \n",
    "\n",
    "            previous_epoch = microbiota_train_set.epochs_completed\n",
    "            real_sample_mini_batch = microbiota_train_set.next_batch(mini_batch_size)\n",
    "            current_epoch = microbiota_train_set.epochs_completed\n",
    "            \n",
    "            noisy1 = input_noise_sample(mini_batch_size, noise_dim)\n",
    "            \n",
    "            \"\"\"\n",
    "            print(\"Discriminator data\")\n",
    "            real_string = str(np.sum(real_sample_mini_batch))+\" \"+str(real_sample_mini_batch.shape) \n",
    "            fale_string = str(np.sum(noisy1))+\" \"+str(noisy1.shape) \n",
    "            print(\"Real \"+real_string+\" fake \"+fale_string)\n",
    "            \"\"\"\n",
    "            \n",
    "            dis_var_dict = {train_real_sample  : real_sample_mini_batch,\n",
    "                            train_noise_sample :  noisy1\n",
    "                           }\n",
    "            \n",
    "            D_train_step.run(feed_dict=dis_var_dict)\n",
    "            D_current_cost, _ = sess.run([D_cost_train, clip_D], feed_dict=dis_var_dict)\n",
    "            \n",
    "            # ======> Debug logits <===========\n",
    "            #dis_logit.append(sess.run([D_logit_fake_train], feed_dict=dis_var_dict))\n",
    "            #gen_logit.append(sess.run([D_logit_real_train], feed_dict=dis_var_dict))\n",
    "            dis_logit.append(sess.run([norm_logit_t_fake,  D_logit_fake_train, fake_L1_output ], feed_dict=dis_var_dict))\n",
    "            gen_logit.append(sess.run([norm_logit_t_real,  D_logit_real_train,  real_L1_output], feed_dict=dis_var_dict))\n",
    "            # ======> Debug logits <===========\n",
    "            \n",
    "            train_d_cost_record.append(D_current_cost)\n",
    "            iter_record_d.append(it+1+k)\n",
    "\n",
    "            if current_epoch > previous_epoch :\n",
    "                d_train_epoch_cost.append(D_current_cost)\n",
    "                add_g_record =1\n",
    "                # g_train_epoch_cost.append(G_current_cost)\n",
    "\n",
    "        # End For\n",
    "        noisy2 = input_noise_sample(mini_batch_size, noise_dim)\n",
    "        # Apply the optimization algorithm and update both network parameters\n",
    "        gen_var_dict = {train_noise_sample:noisy2 }\n",
    "        #_, G_current_cost = sess.run([G_train_step, G_cost_train], feed_dict=gen_var_dict)    \n",
    "        G_train_step.run(feed_dict=gen_var_dict)       \n",
    "               \n",
    "        G_current_cost =  sess.run([ G_cost_train], feed_dict=gen_var_dict)   \n",
    "       \n",
    "        #print(G_current_cost)\n",
    "        #print(\"noise: \"+str(np.sum(noisy)))\n",
    "\n",
    "        \n",
    "        G_current_cost = G_current_cost[0]\n",
    "        \n",
    "        train_g_cost_record.append(G_current_cost)\n",
    "\n",
    "        if add_g_record == 1:\n",
    "            g_train_epoch_cost.append(G_current_cost)\n",
    "            add_g_record = 0\n",
    "\n",
    "        iter_record_g.append(it)\n",
    "\n",
    "        if it % (iteration_number//10) == 0 or it == 0:\n",
    "            end = timer()\n",
    "            print(\"Progreso :\"+str(100*it/(1.0*iteration_number))+\" %\")\n",
    "            print(str(end - start)+\" s\")\n",
    "\n",
    "            current_gen_weights = np.sum(sess.run(my_generator.G_W1))\n",
    "            current_dis_weights = np.sum(sess.run(my_discriminator.D_W1))\n",
    "\n",
    "            print(\"\\t Current Gen Weights \" +str(current_gen_weights))\n",
    "            print(\"\\t Current Dis Weights \" +str(current_dis_weights))\n",
    "\n",
    "            print(\"Epochs completed so far \"+str(microbiota_train_set.epochs_completed))\n",
    "\n",
    "            print('\\t Iter: {}'.format(it))\n",
    "            print('\\t D loss: {:.4}'.format(D_current_cost))\n",
    "            print('\\t G_loss: {:.4}'.format(G_current_cost))\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            \"\"\"\n",
    "            print(len(dis_logit[-1][0]))\n",
    "            print(type(dis_logit[-1]))\n",
    "            print(np.sum(dis_logit[-1]))\n",
    "            print(np.asarray(dis_logit[-1]).shape)\n",
    "            print(np.sum(gen_logit[-1]))\n",
    "        \n",
    "            print(np.asarray(gen_logit[-1]).shape)\n",
    "            print(np.mean(gen_logit[-1]))                  \n",
    "            \"\"\"\n",
    "   \n",
    "            print(\"Norm Logit Fake \\t\"+str(np.sum(dis_logit[-1][0])))\n",
    "            print(\"Logit Fake \\t\"+str(np.sum(dis_logit[-1][1])))\n",
    "            print(\"L1 Output Fake \\t\"+str(np.sum(dis_logit[-1][2])))\n",
    "\n",
    "            print(\"Norm Logit Real \\t\"+str(np.sum(gen_logit[-1][0])))\n",
    "            print(\"Logit Real \\t\"+str(np.sum(gen_logit[-1][1])))\n",
    "            print(\"L1 Output Real \\t\"+str(np.sum(gen_logit[-1][2])))\n",
    "            \n",
    "    #saver = tf.train.Saver()\n",
    "    #saver.save(sess, '../results/trained_gan.ckpt')\n",
    "    #little_record.append(sess.run(my_generator.G_W1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FS2ZQ6BDulY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0715494\n",
      "6.0715494\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(little_record[0]))\n",
    "print(np.sum(little_record[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ex11-IW5BaLT"
   },
   "outputs": [],
   "source": [
    "#plt.plot(iter_record_g, g_cost_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZpFLGjN7K-fD"
   },
   "outputs": [],
   "source": [
    "#plt.plot(iter_record_d, d_cost_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3mXPqcE1AOf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JboHGAWFLPP"
   },
   "source": [
    "# Test loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3634
    },
    "colab_type": "code",
    "id": "aqVHrCytXo3Y",
    "outputId": "892e7c47-13aa-4b79-d22b-c14b67a42f19"
   },
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "number_of_fig_per_plot :int = 16\n",
    "#Training Loop\n",
    "counter = 0\n",
    "\n",
    "test_epochs = train_epochs\n",
    "\n",
    "iters_per_epoch = test_set.shape[0]//mini_batch_size\n",
    "\n",
    "test_iter =   test_epochs*iters_per_epoch\n",
    "\n",
    "\n",
    "\n",
    "#test_iter =  (numero_de_iteraciones*test_set.shape[0])//train_set.shape[0]\n",
    "\n",
    "start = timer()\n",
    "\n",
    "test_g_cost_record = []\n",
    "test_d_cost_record = []\n",
    "iter_test_record_g = []\n",
    "iter_test_record_d = []\n",
    "\n",
    " \n",
    "g_test_epoch_cost = []\n",
    "d_test_epoch_cost =[]\n",
    " \n",
    "\n",
    "epoch_record_g = []\n",
    "epoch = []\n",
    "\n",
    "d_iter_ratio: int = 5\n",
    "    \n",
    "print(iters_per_epoch)\n",
    "print(test_iter)\n",
    "print(test_set.shape[0])\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, '/content/trained_gan.ckpt')\n",
    "\n",
    "    for it in range(test_iter):\n",
    "\n",
    "        # Train more the discrimantor    \n",
    "        for k in range(d_iter_ratio):      \n",
    "\n",
    "            #f k%5 == 0 and k != 0:\n",
    "                #pass\n",
    "\n",
    "            real_sample_mini_batch = microbiota_test_set.next_batch(mini_batch_size)\n",
    "            dis_var_dict = {real_sample: real_sample_mini_batch, noise_sample: input_noise_sample(mini_batch_size, noise_dim)}\n",
    "            D_current_test_cost, _ = sess.run([ D_cost_inference, clip_D], feed_dict=dis_var_dict)\n",
    "            test_d_cost_record.append(D_current_test_cost)\n",
    "            iter_test_record_d.append(it+1+k)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # End For\n",
    "\n",
    "\n",
    "\n",
    "        # Apply the optimization algorithm and update both newtwork parameters\n",
    "        gen_var_dict = {noise_sample: input_noise_sample(mini_batch_size, noise_dim)}\n",
    "        temp_g_cost = sess.run([ G_cost_inference], feed_dict=gen_var_dict)\n",
    "        #Since is the single fetch\n",
    "        G_current_test_cost = temp_g_cost[0]\n",
    "\n",
    "        test_g_cost_record.append(G_current_test_cost)\n",
    "\n",
    "        iter_test_record_g.append(it)\n",
    "\n",
    "        if it%iters_per_epoch == 0 and (k !=0):\n",
    "            d_test_epoch_cost.append(D_current_test_cost)\n",
    "            g_test_epoch_cost.append(G_current_test_cost)\n",
    "\n",
    "        if it % (test_iter//10) == 0:\n",
    "            end = timer()\n",
    "            print(\"Progreso :\"+str(100*it/(1.0*test_iter))+\" %\")\n",
    "            print(str(end - start)+\" s\")\n",
    "\n",
    "            print(\"epochs completed \"+str(microbiota_test_set._epochs_completed))\n",
    "            print(it/(iters_per_epoch/5))\n",
    "\n",
    "\n",
    "            print('\\t Iter: {}'.format(it))\n",
    "            print('\\t D loss: {:.4}'.format(D_current_test_cost))\n",
    "            print('\\t G_loss: {:.4}'.format(G_current_test_cost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49_9DZMcIbgv"
   },
   "source": [
    "# Generator Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_oc7LTVDul4"
   },
   "outputs": [],
   "source": [
    "iter_line = list(range(len(train_g_cost_record)))\n",
    "test_iter_line = list(range(len(test_g_cost_record)))\n",
    "\n",
    "epochs_list = list(range(len(g_test_epoch_cost)))\n",
    "\n",
    "\n",
    "#plt.plot(iter_line, train_g_cost_record )\n",
    "#plt.plot(test_iter_line, test_g_cost_record)\n",
    "\n",
    "plt.plot(epochs_list, g_train_epoch_cost)\n",
    "\n",
    "plt.plot(epochs_list, g_test_epoch_cost)\n",
    "\n",
    "plt.legend(['train set', 'test set'], loc='upper right')\n",
    "plt.title(\"Generator Loss\")\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "#train_g_cost_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ay6vrmOKmy49"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs_list, d_train_epoch_cost)\n",
    "\n",
    "plt.plot(epochs_list, d_test_epoch_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5q0l-xBFIirH"
   },
   "source": [
    "## Closer look on the interval [0,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjq7YAS8Dul9"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Generator loss on test set\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(iter_line[0:100],test_g_cost_record[0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4-96NYrIpAp"
   },
   "source": [
    "# Discriminator Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuaNXSOFJDpd"
   },
   "outputs": [],
   "source": [
    "iter_line_d = list(range(len(train_d_cost_record)))\n",
    "\n",
    "#plt.plot(iter_line_d,train_d_cost_record )\n",
    "#plt.plot(iter_line_d,test_d_cost_record)\n",
    "\n",
    "\n",
    "plt.plot(epochs_list, d_train_epoch_cost)\n",
    "\n",
    "plt.plot(epochs_list, d_test_epoch_cost)\n",
    "\n",
    "\n",
    "plt.legend(['train set', 'test set'], loc='upper right')\n",
    "plt.title(\"Discriminator Loss\")\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "#train_g_cost_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VRGMp249IuN8"
   },
   "source": [
    "## Closer look on the interval [0,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nP7zkua_Ef0"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Discriminator loss on test set\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(iter_line_d[0:100],test_d_cost_record[0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8nR0N42uI08m"
   },
   "source": [
    "# Saving the model and othe file management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4J354qa-4Yn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTW2G1O4EWpr"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Comprimimos todos los archivos que se regenraron al guardar el moelo y despues lo descargamos\n",
    "!zip  bio_gan.zip bio_gan.*\n",
    "\n",
    "files.download(\"bio_gan.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1INUu5SOn0T"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Shj51eukUQtF"
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, '/content/trained_gan.ckpt')\n",
    "    \n",
    "    w1 = sess.run(my_generator.G_W1)\n",
    "    b1 = sess.run(my_generator.G_b1)\n",
    "\n",
    "    w2= sess.run(my_generator.G_W2)\n",
    "    b2 = sess.run(my_generator.G_b2)\n",
    "\n",
    "\n",
    "\n",
    "gen_weights_and_bias = [w1, b1, w2, b2]\n",
    "\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwbbsHUkXGvV"
   },
   "outputs": [],
   "source": [
    "#import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goUEpGQ2X192"
   },
   "outputs": [],
   "source": [
    "with open('gen_param.pkl', 'wb') as f:\n",
    "  pickle.dump(gen_weights_and_bias, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xw-sbXW-I8JH"
   },
   "source": [
    "# Using the Generator to create some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "njm8PHdMTfWD",
    "outputId": "74ae8f1e-f6f5-4979-d9d9-deea7794fb52"
   },
   "outputs": [],
   "source": [
    "# generator_sample_inference = my_generator.inference_draw_samples(noise_sample)\n",
    "\n",
    "noise_sample = tf.placeholder(tf.float32, shape=[None, noise_dim])\n",
    "sample_nara = my_generator.inference_draw_samples(noise_sample)\n",
    "\n",
    "    \n",
    "n_samples = 10\n",
    "samples_table = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver.restore(sess, '/content/trained_gan.ckpt')\n",
    "\n",
    "    for k in range(n_samples):\n",
    "        \n",
    "        magico = input_noise_sample(1, 10)\n",
    "        print(np.sum(magico))\n",
    "\n",
    "        \n",
    "        f_sam = sess.run(sample_nara, feed_dict={noise_sample :magico})\n",
    "        samples_table.append(f_sam[0] )\n",
    "        done_per =(k/(1.0*n_samples))*100\n",
    "        #print(str(round(done_per,2))+\"% has samples have been created\")\n",
    "        #print(k)\n",
    "        if k%(n_samples//10)==0:\n",
    "            print(str(done_per)+\"% has samples have been created\")\n",
    "\n",
    "samples_table = np.array(samples_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "bSARemagVTjZ",
    "outputId": "58678351-1582-433e-f69d-155caade7b79"
   },
   "outputs": [],
   "source": [
    "sumatorias = [ np.sum(samples_table[k,:]) for k in range(samples_table.shape[0])]\n",
    "\n",
    "prome = [ np.mean(samples_table[k,:]) for k in range(samples_table.shape[0])]\n",
    "print(sumatorias[0:5])\n",
    "print(prome[0:5])\n",
    "print(len(prome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUQPh5Oz3ELb"
   },
   "outputs": [],
   "source": [
    "samples_table = np.array(samples_table) \n",
    "    \n",
    "    \n",
    "ts_min = []\n",
    "for row in range(samples_table.shape[0]):\n",
    "    ts_min.append(np.min(samples_table[row,:]))\n",
    "    \n",
    "ts_max = []\n",
    "for row in range(samples_table.shape[0]):\n",
    "    ts_max.append(np.max(samples_table[row,:]))\n",
    "    \n",
    "ts_max = np.array(ts_max)\n",
    "ts_min = np.array(ts_min)\n",
    "    \n",
    "new_samples_table =  []\n",
    "for row in range(samples_table.shape[0]):\n",
    "\n",
    "    _ = samples_table[row,:]*(ts_max[row]-ts_min[row])+ts_min[row]\n",
    "    new_samples_table.append(_)\n",
    "      \n",
    "new_samples_table = np.array(new_samples_table)\n",
    "\n",
    " \n",
    "\n",
    "glv_gen_errors = []\n",
    "for k in range(samples_table.shape[0]):\n",
    "    error = np.sum( GLV_Model(samples_table[k,:], m_A, m_r ))\n",
    "    #print(error)\n",
    "    glv_gen_errors.append(error)\n",
    "    \n",
    "glv_gen_errors = np.array(glv_gen_errors)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "I78sINCl4IaI",
    "outputId": "47218032-2ae7-4784-ff6c-26504b684559"
   },
   "outputs": [],
   "source": [
    "print(\"GAN Samples: \"+str(samples_table.shape[0]))\n",
    "print(\"GAN mean GLV error : \"+str(np.mean(glv_gen_errors)))\n",
    "print(\"GAN GLV error std : \"+str(np.std(glv_gen_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fjXDx22yjuA"
   },
   "outputs": [],
   "source": [
    "sub = DataSetManager(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "umLiU5doK38o",
    "outputId": "789db967-e1c3-46f1-9909-97b93b5e0b3c"
   },
   "outputs": [],
   "source": [
    "sub_train_set = sub.next_batch(samples_table.shape[0]) #train_set[0:n_samples,:]\n",
    "\n",
    "glv_train_errors = []\n",
    "for k in range(sub_train_set.shape[0]):\n",
    "    error = np.sum( GLV_Model(sub_train_set[k,:], m_A, m_r ))\n",
    "    #print(error)\n",
    "    glv_train_errors.append(error)\n",
    "    \n",
    "glv_train_errors = np.array(glv_train_errors)\n",
    "\n",
    "glv_mean_e = np.round(np.mean(glv_train_errors),4)\n",
    "glv_e_std = np.round(np.std(glv_train_errors),4)\n",
    "\n",
    "\n",
    "print(\"Training Samples <Normalized>: \"+str(samples_table.shape[0]))\n",
    "print(\"Sub Train Set mean GLV error : \"+str(glv_mean_e))\n",
    "print(\"Sub Train Set error std : \"+str(glv_e_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "b58KIIKSr2HU",
    "outputId": "f3629e94-fe2f-4a62-984a-dabdecd0d175"
   },
   "outputs": [],
   "source": [
    "diff_e = np.mean(glv_gen_errors)  - glv_mean_e \n",
    "diff_std = np.std(glv_gen_errors) - glv_e_std\n",
    "\n",
    "print(\"Samples: \"+str(samples_table.shape[0]))\n",
    "\n",
    "print(\"GAN-Train mean GLV error : \"+str(diff_e))\n",
    "print(\"GAN-Train GLV error std : \"+str(diff_std))\n",
    "\n",
    "print(\"% GAN-Train mean GLV error : \"+str(100*diff_e/glv_mean_e))\n",
    "print(\"% GAN-Train GLV error std : \"+str(100*diff_std/glv_e_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PoEucmecGbC7",
    "outputId": "1f6a554c-5ebb-4e1a-c483-e2c9cb3988b1"
   },
   "outputs": [],
   "source": [
    "np.sum(samples_table < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "_Jj1pa0S876y",
    "outputId": "23713a29-57c9-4b91-a7ff-e213b08e1fc4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(samples_table)\n",
    "with open('unormed.csv', 'a') as f:\n",
    "    df.to_csv(f, header=False, index=False)\n",
    "    \n",
    "    \n",
    "\n",
    "df = pd.DataFrame(new_samples_table)\n",
    "with open('normalized.csv', 'a') as f:\n",
    "    df.to_csv(f, header=False, index=False)    \n",
    "    \n",
    "!zip gan_samples.zip *.csv\n",
    "\n",
    "files.download(\"gan_samples.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPVK_CpdJ3PG"
   },
   "source": [
    "## Asessing the GLV error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "colab_type": "code",
    "id": "QC0KpZVuJ6U_",
    "outputId": "a471b65c-6363-4799-bc01-c799d489cb70"
   },
   "outputs": [],
   "source": [
    "d = {'GAN': glv_gen_errors, 'Training': glv_train_errors }\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "ax = sns.boxplot( data=df)\n",
    "\n",
    "plt.title(\"GLV Error, \"+str(train_epochs)+\" epochs , \"+str(n_samples)+\" samples draw\")\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.savefig(\"glv_barplot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37FZvm3nVi9b"
   },
   "outputs": [],
   "source": [
    "# !rm *.png\n",
    "files.download(\"glv_barplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWAaB-TVJeqk"
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HR6j1leRseLR"
   },
   "outputs": [],
   "source": [
    "sns.barplot(x = list(range(1,51)) ,y= np.sum(normalize_ds(samples_table), axis=0))\n",
    "plt.title(\"GAN-Dataset Community Normalized Species Abundance\")\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Abundance\")\n",
    "plt.savefig(\"fake.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIDA0nB9DyJ1"
   },
   "outputs": [],
   "source": [
    "\n",
    "rnd_index = np.random.randint(low=1, high=140000,size=100)\n",
    "sub_ds = normalize_ds(microbiota_table)[rnd_index]\n",
    "\n",
    "sns.barplot(x = list(range(1,29)) ,y= np.sum(sub_ds, axis=0))\n",
    "plt.title(\"Dataset Community Normalized Species Abundance\")\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Abundance\")\n",
    "plt.savefig(\"real.svg\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2hauKEGE7BE"
   },
   "outputs": [],
   "source": [
    "files.download(\"real.svg\")\n",
    "files.download(\"fake.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uk7ifX9CFTcr"
   },
   "outputs": [],
   "source": [
    "a_t = normalize_ds(samples_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFWi05ZcG1Wb"
   },
   "outputs": [],
   "source": [
    "show_rounded_array(a_t[8,:],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcuBXB2AIhaM"
   },
   "outputs": [],
   "source": [
    "!zip  pics.zip *.svg\n",
    "files.download(\"pics.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoON-tMqImcJ"
   },
   "outputs": [],
   "source": [
    "!head -n 20 MicrobiotaGAN/generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ENcgkJGVAmK"
   },
   "outputs": [],
   "source": [
    "bool(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lRaScDq2XOso"
   },
   "outputs": [],
   "source": [
    "dir(my_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dzoenDufTKZ"
   },
   "outputs": [],
   "source": [
    "def Diagonal_Matrix(input_vector):\n",
    "     return np.multiply(np.identity(input_vector.shape[0]),input_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6CjTaa9emxm"
   },
   "outputs": [],
   "source": [
    "s_0 = normalize_ds(microbiota_table)[0]\n",
    "\n",
    "res = np.multiply(Diagonal_Matrix(s_0), np.matmul(m_A,s_0)+m_rho)\n",
    "\n",
    "print(res[0,:])\n",
    "\n",
    "plt.imshow(res, cmap=\"gray\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(m_A.shape)\n",
    "print(s_0.shape)\n",
    "print(s_0)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q85vkt2KBQwG"
   },
   "outputs": [],
   "source": [
    "!head -n 25 MicrobiotaGAN/glv_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWb-8xsCKf__"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bio_WGAN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:idp]",
   "language": "python",
   "name": "conda-env-idp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
